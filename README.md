<div align="center">

# ğŸ¡ King County Housing Market Analysis

*Predictive modeling of residential property values in King County, Washington*

**Ironhack Data Science Bootcamp - February 2026**

[View Presentation](./presentation/King_countyHousePrice_presentation.pdf) â€¢ [Explore Notebooks](./notebooks/)

---

</div>

## ğŸ¯ Project Goal

Build machine learning models to accurately predict house prices in King County by analyzing property characteristics, location data, and market trends from 21,613 home sales.

## ğŸ“– Table of Contents

- [About the Data](#-about-the-data)
- [Our Approach](#-our-approach)
- [What We Built](#-what-we-built)
- [Results](#-results)
- [Technologies](#-technologies)
- [Getting Started](#-getting-started)

---

## ğŸ—‚ï¸ About the Data

**Source:** [Kaggle - King County House Sales](https://www.kaggle.com/datasets/minasameh55/king-country-houses-aa)

Our dataset contains **21,613 residential property transactions** with comprehensive information about each home:

### Property Features
- **Physical attributes**: bedrooms, bathrooms, square footage (living/lot/basement/above)
- **Quality indicators**: condition rating, grade, number of floors
- **Location data**: latitude, longitude, zipcode
- **Special features**: waterfront property, view quality
- **History**: year built, renovation year
- **Neighborhood context**: nearby properties' characteristics (15 neighbors)

### Data Characteristics
âœ… Complete dataset - no missing values  
âš ï¸ Some properties appear multiple times (repeat sales)  
ğŸ“Š Right-skewed price distribution  
ğŸ—ºï¸ Clear geographic pricing patterns

---

## ğŸ”¬ Our Approach

### 1ï¸âƒ£ **Exploratory Data Analysis**

We started by understanding the data landscape:

- Mapped geographic price distributions across King County
- Identified outliers in square footage and bedroom counts
- Discovered properties with questionable data (e.g., 0 bedrooms and 0 bathrooms)
- Analyzed price correlations with property features

**Key Discovery:** Location and living space dominate pricing, but luxury features create significant premiums.

### 2ï¸âƒ£ **Data Preparation**

Cleaned and transformed the raw data:

```python
# Examples of our preprocessing
- Removed invalid entries (0 bedrooms and 0 bathrooms)
- Applied winsorization to handle extreme outliers
- Split sale dates into temporal features
```

### 3ï¸âƒ£ **Feature Engineering**

Created meaningful features from raw data:

**Geographic Features**
- `loc_clusters` - Neighborhood groupings from coordinates
- `dist_seattle` - Proximity to downtown

**Property Metrics**
- `renovated` - Boolean: has the house been updated?
- `house_age` - Years since construction
- `total_living_ratio` - Indoor space relative to lot size
- `relative_size` - Comparison to neighboring properties

**Quality Indicators**
- `luxury_index` - Composite score: grade + view + 2Ã—waterfront
- `quality_interaction` - Grade multiplied by condition
- `size_grade_interaction` - Large homes with high quality

**Sales History**
- `sold_occ` - Number of times property was sold

### 4ï¸âƒ£ **Model Development**

Tested multiple algorithms with iterative improvement:

**Baseline â†’ Linear Regression**  
Established performance floor

**Ensemble Methods â†’ XGBoost, CatBoost, Gradient Boosting**  
Leveraged tree-based models for complex patterns

**Optimization â†’ Hyperparameter Tuning**  
GridSearchCV + manual fine-tuning

---

## ğŸ—ï¸ What We Built

### Model Portfolio

#### **Linear Regression (Baseline)**
Simple interpretable model for benchmarking
- RÂ² = 0.703 (baseline) â†’ 0.753 (with features)
- Validates feature engineering impact

#### **XGBoost Regressor** â­
Best overall performance with full feature set
- Test RÂ² = **0.917**
- GridSearchCV â†’ Optimal params: max_depth=7, learning_rate=0.1, n_estimators=300
- Experimented with dimensionality reduction (Top 10 features + PLS)

#### **CatBoost Regressor** â­
Top performer for categorical feature handling
- Test RÂ² = **0.917** | MAE = **57,474** (best)
- Native support for categorical variables
- Depth=10, learning_rate=0.015, n_estimators=5000

#### **Gradient Boosting Regressor**
Systematic tuning approach
- RandomizedSearchCV â†’ Manual optimization
- Reduced overfitting while maintaining RÂ² = 0.905
- Best for understanding hyperparameter effects

---

## ğŸ“Š Results

### Performance Comparison

| Model | Test RÂ² | RMSE | MAE | Highlight |
|-------|---------|------|-----|-----------|
| Linear (baseline) | 0.703 | 201,461 | 124,749 | Starting point |
| Linear (engineered) | 0.753 | 158,109 | 108,573 | +7% improvement |
| **XGBoost** | **0.917** | 91,876 | 58,869 | ğŸ¥‡ Tied winner |
| **CatBoost** | **0.917** | 94,482 | **57,474** | Lowest MAE |
| GradientBoosting | 0.905 | 112,792 | 62,105 | Best generalization |

### What Matters Most?

**Top 5 Feature Importance (XGBoost)**
1. ğŸ  **sqft_living** - Interior space is king
2. ğŸŒŠ **waterfront** - Massive price premium
3. ğŸ‘ï¸ **view** - View quality drives value
4. â­ **grade** - Construction quality rating
5. ğŸ“ **sqft_living15** - Neighborhood context

### Key Insights

ğŸ’¡ **Feature engineering delivered** - 7% RÂ² boost for linear models  
ğŸ’¡ **Ensemble methods dominated** - 21% improvement over baseline  
ğŸ’¡ **Location matters enormously** - Geographic features critical  
ğŸ’¡ **CatBoost excels with mixed data** - Best for categorical handling  

---

## ğŸ› ï¸ Technologies

```python
# Core Stack
Python 3.9
pandas, numpy          # Data manipulation
matplotlib, seaborn    # Visualization

# Machine Learning
scikit-learn          # Preprocessing, linear models, validation
xgboost              # Gradient boosting
catboost             # Categorical boosting
```

**Development Environment:** Jupyter Notebook  
**Data Source:** Kaggle API (`kagglehub`)

---

## ğŸš€ Getting Started

### Quick Start

```python
# 1. Download the data
import kagglehub
path = kagglehub.dataset_download("minasameh55/king-country-houses-aa")

# 2. Run the EDA notebook
jupyter notebook notebooks/king-county-house-prices-eda.ipynb

# 3. Experiment with models
jupyter notebook notebooks/Xgboost_regressor_experiments.ipynb
```

### Project Structure

```
king-county-house-prices/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ df_fe.csv          # processed dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ king-county-house-prices-eda.ipynb        # Full EDA
â”‚   â””â”€â”€ Xgboost_regressor_experiments.ipynb       # Model 
â”‚
â”œâ”€â”€ presentation/
â”‚   â””â”€â”€ King_countyHousePrice_presentation.pdf    # Final 
â”‚
â””â”€â”€ README.md
```
---

## ğŸ“œ License & Acknowledgments

**Data:** [King County House Sales dataset](https://www.kaggle.com/datasets/minasameh55/king-country-houses-aa) via Kaggle

This work was built together with [@alexandrade1978](https://github.com/alexandrade1978) and [@MariusGoeren](https://github.com/MariusGoeren) ğŸ™Œ

---

<div align="center">


[â¬† Back to Top](#-king-county-housing-market-analysis)

</div>